Speech audio emotion recognition is an emerging field in audio signal processing that aims to automatically identify and classify emotions conveyed in speech signals. This project presents a speech audio emotion recognition system using Deep Learning Techniques. The system utilizes a combination of acoustic features, including Mel Frequency Cepstral Coefficients, Chroma, Mel spectrogram, Spectral Roll off, Spectral flux, and Zero-crossing rate, to capture the relevant characteristics of speech signals associated with different emotions.
The project follows a systematic workflow consisting of data analysis, preprocessing, feature extraction, dataset splitting, model training, and evaluation. The speech samples are preprocessed and the desired features are extracted using established techniques. The Multi-Layer Perceptron classifier is designed with configurable parameters and trained on the training subset using the extracted features and emotion labels. The trained model is evaluated on the testing subset to predict the emotions conveyed in speech samples. Experimental results on a selected dataset demonstrate the promising accuracy of the proposed approach in classifying. The project provides a robust approach for automatically identifying and classifying emotions in speech signals. The project contributes to the field of audio signal processing and offers potential applications in various domains requiring emotion recognition capabilities.
